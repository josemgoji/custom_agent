{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b751c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ee1b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Cargar el tokenizer y el modelo manualmente\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    token=hf_token\n",
    ")\n",
    "# Si no tiene pad_token, lo asignamos al eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14ea8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Crear el pipeline de HuggingFace\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "# Integrar con LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c5dd602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is Hugging Face? Hugging Face is an open-source software library for natural language processing (NLP) tasks. It was created by Hugging Face LLC and is widely used in the field of NLP. The library provides a wide range of pre-trained models and tools for tasks such as text classification, sentiment analysis, and language translation.\\nHugging Face provides a simple and intuitive API for accessing and using these models, making it easy for developers to integrate NLP capabilities into their applications. The library also includes a range'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is Hugging Face?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168bc442",
   "metadata": {},
   "source": [
    "# Nodo inicial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b71ef35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modo detectado: modo guiado\n",
      "Modo detectado: modo guiado\n",
      "Modo detectado: modo libre\n",
      "Modo detectado: modo libre\n",
      "Modo detectado: modo libre\n",
      "Modo detectado: modo guiado\n",
      "Modo detectado: modo libre\n",
      "Modo detectado: modo guiado\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Paso 1: Define el esquema de estado\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    mode: str\n",
    "\n",
    "# Paso 2: Define el prompt para clasificar el modo\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=(\n",
    "        \"Clasifica el siguiente mensaje del usuario como 'modo libre' o 'modo guiado'. \"\n",
    "        \"Solo responde con una de esas dos opciones.\\n\\n\"\n",
    "        \"Mensaje: {input}\\n\"\n",
    "        \"Clasificación:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Paso 3: Crea la cadena de clasificación\n",
    "classifier_chain = LLMChain(\n",
    "    llm=llm,  # Tu LLM de HuggingFacePipeline\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Paso 4: Define la función de clasificación (corregida)\n",
    "def classify_mode(state):\n",
    "    user_input = state[\"input\"]\n",
    "    result = classifier_chain.run(input=user_input)\n",
    "    # Buscar la línea que contiene 'Clasificación:'\n",
    "    mode = \"modo guiado\"  # Valor por defecto\n",
    "    for line in result.splitlines():\n",
    "        if \"clasificación:\" in line.lower():\n",
    "            after_colon = line.split(\":\", 1)[-1].strip().lower()\n",
    "            if \"libre\" in after_colon:\n",
    "                mode = \"modo libre\"\n",
    "            else:\n",
    "                mode = \"modo guiado\"\n",
    "            break\n",
    "    state[\"mode\"] = mode\n",
    "    return state\n",
    "\n",
    "# Paso 5: Crea el grafo de LangGraph\n",
    "graph = StateGraph(state_schema=AgentState)\n",
    "graph.add_node(\"clasificar\", classify_mode)\n",
    "graph.set_entry_point(\"clasificar\")\n",
    "graph.add_edge(\"clasificar\", END)\n",
    "\n",
    "# Paso 6: Compila el grafo\n",
    "app = graph.compile()\n",
    "\n",
    "# Paso 7: Usa el agente con input desde teclado\n",
    "while True:\n",
    "    user_input = input(\"Escribe tu mensaje (o 'salir' para terminar): \")\n",
    "    if user_input.lower() == \"salir\":\n",
    "        break\n",
    "    result = app.invoke({\"input\": user_input})\n",
    "    print(\"Modo detectado:\", result[\"mode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f27dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    mode: str\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=(\n",
    "        \"Clasifica el siguiente mensaje del usuario como 'modo libre' o 'modo guiado'. \"\n",
    "        \"Solo responde con una de esas dos opciones.\\n\\n\"\n",
    "        \"Mensaje: {input}\\n\"\n",
    "        \"Clasificación:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"o4-mini\",\n",
    "    temperature=1,\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "classifier_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61ee7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = classifier_chain.invoke(input=\"quiero solo preguntar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36338e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'quiero solo preguntar', 'text': 'modo libre'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3ae005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'quiero ir paso a paso', 'text': 'modo guiado'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta = classifier_chain.invoke(input=\"quiero ir paso a paso\")\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140e3710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgome\\AppData\\Local\\Temp\\ipykernel_25344\\4117726483.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = classifier_chain.run(input=user_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modo detectado: modo guiado\n",
      "Modo detectado: modo guiado\n",
      "Modo detectado: modo guiado\n",
      "Modo detectado: modo guiado\n"
     ]
    }
   ],
   "source": [
    "def classify_mode(state):\n",
    "    user_input = state[\"input\"]\n",
    "    result = classifier_chain.run(input=user_input)\n",
    "    mode = \"modo guiado\"\n",
    "    for line in result.splitlines():\n",
    "        if \"clasificación:\" in line.lower():\n",
    "            after_colon = line.split(\":\", 1)[-1].strip().lower()\n",
    "            if \"libre\" in after_colon:\n",
    "                mode = \"modo libre\"\n",
    "            else:\n",
    "                mode = \"modo guiado\"\n",
    "            break\n",
    "    state[\"mode\"] = mode\n",
    "    return state\n",
    "\n",
    "graph = StateGraph(state_schema=AgentState)\n",
    "graph.add_node(\"clasificar\", classify_mode)\n",
    "graph.set_entry_point(\"clasificar\")\n",
    "graph.add_edge(\"clasificar\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Escribe tu mensaje (o 'salir' para terminar): \")\n",
    "    if user_input.lower() == \"salir\":\n",
    "        break\n",
    "    result = app.invoke({\"input\": user_input})\n",
    "    print(\"Modo detectado:\", result[\"mode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d07c2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modo detectado: modo libre\n",
      "Iniciando modo libre: puedes preguntar lo que quieras.\n",
      "Modo detectado: modo libre\n",
      "Iniciando modo libre: puedes preguntar lo que quieras.\n",
      "Modo detectado: modo guiado\n",
      "Iniciando modo guiado: te explicaré paso a paso.\n",
      "Modo detectado: modo guiado\n",
      "Iniciando modo guiado: te explicaré paso a paso.\n",
      "Modo detectado: modo guiado\n",
      "Iniciando modo guiado: te explicaré paso a paso.\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Esquema de estado\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    mode: str\n",
    "    output: str\n",
    "\n",
    "# Prompt para clasificar el modo\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=(\n",
    "        \"Clasifica el siguiente mensaje del usuario como 'modo libre' o 'modo guiado'. \"\n",
    "        \"Solo responde con una de esas dos opciones.\\n\\n\"\n",
    "        \"Mensaje: {input}\\n\"\n",
    "        \"Clasificación:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# LLM de OpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=\"o4-mini-2025-04-16\",\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Cadena de clasificación\n",
    "classifier_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Nodo de clasificación\n",
    "def classify_mode(state):\n",
    "    user_input = state[\"input\"]\n",
    "    respuesta = classifier_chain.invoke(input=user_input)\n",
    "    modo = respuesta.get(\"text\", \"\").strip().lower()\n",
    "    if \"libre\" in modo:\n",
    "        state[\"mode\"] = \"modo libre\"\n",
    "    else:salir\n",
    "        state[\"mode\"] = \"modo guiado\"\n",
    "    return state\n",
    "\n",
    "# Nodo para modo guiado\n",
    "def modo_guiado(state):\n",
    "    # Aquí puedes poner el flujo guiado real, por ahora solo un mensaje\n",
    "    state[\"output\"] = \"Iniciando modo guiado: te explicaré paso a paso.\"\n",
    "    return state\n",
    "\n",
    "# Nodo para modo libre\n",
    "def modo_libre(state):\n",
    "    # Aquí puedes poner el flujo libre real, por ahora solo un mensaje\n",
    "    state[\"output\"] = \"Iniciando modo libre: puedes preguntar lo que quieras.\"\n",
    "    return state\n",
    "\n",
    "# Construcción del grafo\n",
    "graph = StateGraph(state_schema=AgentState)\n",
    "graph.add_node(\"clasificar\", classify_mode)\n",
    "graph.add_node(\"guiado\", modo_guiado)\n",
    "graph.add_node(\"libre\", modo_libre)\n",
    "\n",
    "# Flujo: clasificar -> guiado/libre -> END\n",
    "graph.set_entry_point(\"clasificar\")\n",
    "graph.add_conditional_edges(\n",
    "    \"clasificar\",\n",
    "    lambda state: state[\"mode\"],\n",
    "    {\n",
    "        \"modo guiado\": \"guiado\",\n",
    "        \"modo libre\": \"libre\"\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"guiado\", END)\n",
    "graph.add_edge(\"libre\", END)\n",
    "\n",
    "# Compilar el grafo\n",
    "app = graph.compile()\n",
    "\n",
    "# Loop de interacción\n",
    "while True:\n",
    "    user_input = input(\"Escribe tu mensaje (o 'salir' para terminar): \")\n",
    "    if user_input.lower() == \"salir\":\n",
    "        break\n",
    "    result = app.invoke({\"input\": user_input, \"mode\": \"\", \"output\": \"\"})\n",
    "    print(\"Modo detectado:\", result[\"mode\"])\n",
    "    print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb83426",
   "metadata": {},
   "source": [
    "## Nodo Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f179c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vamos a hacer un quiz para conocer tu nivel.\n",
      "\n",
      "--- Diagnóstico del agente ---\n",
      "{\"nivel\":\"intermedio\",\"fortalezas\":[\"Desviación estándar\"],\"debilidades\":[\"Variables aleatorias\",\"Distribución binomial\"],\"detalle\":[{\"pregunta\":\"¿Qué es una variable aleatoria?\",\"respuesta\":\"una variable que puede tomar diferentes valores\",\"tema\":\"Variables aleatorias\",\"puntaje\":2,\"feedback\":\"Definición muy general: no menciona la relación con experimentos aleatorios ni la asignación de probabilidades.\"},{\"pregunta\":\"¿Qué es la desviación estándar?\",\"respuesta\":\"que tan alejados estan los datos de la media\",\"tema\":\"Desviación estándar\",\"puntaje\":4,\"feedback\":\"Buena descripción conceptual de la dispersión; se podrían añadir detalles formales como la raíz cuadrada de la varianza.\"},{\"pregunta\":\"¿Qué es una distribución binomial?\",\"respuesta\":\"una distribuccion que se base en que los acontecimientos son exito o frcaso, 0 o 1\",\"tema\":\"Distribuciones avanzadas\",\"puntaje\":2,\"feedback\":\"Identifica el carácter binario; falta mencionar el número de ensayos, la función de masa de probabilidad y el parámetro de probabilidad de éxito.\"}]}\n",
      "\n",
      "Resumen final:\n",
      "Nivel: \n",
      "Fortalezas: ['']\n",
      "Debilidades: ['']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import openai\n",
    "import json\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 1. Clase Estado simple\n",
    "class Estado:\n",
    "    def __init__(self):\n",
    "        self.nivel = None\n",
    "        self.fortalezas = []\n",
    "        self.debilidades = []\n",
    "\n",
    "# 2. Preguntas por nivel y tema\n",
    "quiz_preguntas = {\n",
    "    \"basico\": [\n",
    "        {\"tema\": \"Variables aleatorias\", \"pregunta\": \"¿Qué es una variable aleatoria?\"},\n",
    "        {\"tema\": \"Probabilidad\", \"pregunta\": \"¿Qué es la probabilidad clásica?\"},\n",
    "        {\"tema\": \"Distribuciones simples\", \"pregunta\": \"¿Qué es una distribución uniforme?\"},\n",
    "        {\"tema\": \"Eventos\", \"pregunta\": \"¿Qué es un evento en probabilidad?\"}\n",
    "    ],\n",
    "    \"intermedio\": [\n",
    "        {\"tema\": \"Desviación estándar\", \"pregunta\": \"¿Qué es la desviación estándar?\"},\n",
    "        {\"tema\": \"Medidas de dispersión\", \"pregunta\": \"¿Qué es la varianza?\"},\n",
    "        {\"tema\": \"Estadística descriptiva\", \"pregunta\": \"¿Qué es la media aritmética?\"},\n",
    "        {\"tema\": \"Distribuciones\", \"pregunta\": \"¿Qué es una distribución normal?\"}\n",
    "    ],\n",
    "    \"avanzado\": [\n",
    "        {\"tema\": \"Probabilidad conjunta\", \"pregunta\": \"¿Cómo se calcula la probabilidad conjunta de dos eventos independientes?\"},\n",
    "        {\"tema\": \"Teorema de Bayes\", \"pregunta\": \"Explica el teorema de Bayes con un ejemplo.\"},\n",
    "        {\"tema\": \"Distribuciones avanzadas\", \"pregunta\": \"¿Qué es una distribución binomial?\"},\n",
    "        {\"tema\": \"Inferencia\", \"pregunta\": \"¿Qué es una estimación puntual en inferencia estadística?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 3. Template del prompt\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"Eres un experto en educación. Evalúa las siguientes respuestas del usuario a preguntas de probabilidad y estadística.\n",
    "Para cada respuesta, califica de 0 a 5 (donde 0 es incorrecta y 5 es perfecta), explica brevemente la calificación.\n",
    "Al final, resume las fortalezas y debilidades del usuario por tema y sugiere el nivel adecuado (básico, intermedio, avanzado) según el promedio de los puntajes:\n",
    "- Básico: promedio < 2.5\n",
    "- Intermedio: 2.5 <= promedio < 4\n",
    "- Avanzado: promedio >= 4\n",
    "\n",
    "Devuelve la respuesta SOLO en formato JSON con la siguiente estructura:\n",
    "{{\n",
    "  \"nivel\": \"basico/intermedio/avanzado\",\n",
    "  \"fortalezas\": [\"tema1\", \"tema2\", ...],\n",
    "  \"debilidades\": [\"tema1\", \"tema2\", ...],\n",
    "  \"detalle\": [\n",
    "    {{\n",
    "      \"pregunta\": \"...\",\n",
    "      \"respuesta\": \"...\",\n",
    "      \"tema\": \"...\",\n",
    "      \"puntaje\": 0-5,\n",
    "      \"feedback\": \"...\"\n",
    "    }},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Respuestas del usuario:\n",
    "{respuestas_usuario}\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# 4. Nodo de quiz de nivel\n",
    "def nodo_quiz_nivel(state):\n",
    "    print(\"\\nVamos a hacer un quiz para conocer tu nivel.\")\n",
    "    respuestas_usuario = []\n",
    "    for nivel, preguntas in quiz_preguntas.items():\n",
    "        seleccionadas = random.sample(preguntas, 1)\n",
    "        for q in seleccionadas:\n",
    "            resp = input(q[\"pregunta\"] + \" (responde brevemente): \")\n",
    "            respuestas_usuario.append({\n",
    "                \"nivel\": nivel,\n",
    "                \"tema\": q[\"tema\"],\n",
    "                \"pregunta\": q[\"pregunta\"],\n",
    "                \"respuesta\": resp\n",
    "            })\n",
    "    # Prepara el prompt usando el template\n",
    "    prompt = prompt_template.format(respuestas_usuario=str(respuestas_usuario))\n",
    "    # Llama al LLM\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"o4-mini-2025-04-16\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    diagnostico = response.choices[0].message.content\n",
    "    print(\"\\n--- Diagnóstico del agente ---\")\n",
    "    print(diagnostico)\n",
    "    resultado = json.loads(diagnostico)\n",
    "    state.nivel = resultado[\"nivel\"]\n",
    "    state.fortalezas = resultado[\"fortalezas\"]\n",
    "    state.debilidades = resultado[\"debilidades\"]\n",
    "    state.detalle = resultado[\"detalle\"]\n",
    "    print(f\"\\nNivel detectado: {state.nivel}\")\n",
    "    print(f\"Fortalezas: {state.fortalezas}\")\n",
    "    print(f\"Debilidades: {state.debilidades}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# 5. Prueba el nodo\n",
    "if __name__ == \"__main__\":\n",
    "    estado = Estado()\n",
    "    nodo_quiz_nivel(estado)\n",
    "    print(\"\\nResumen final:\")\n",
    "    print(\"Nivel:\", estado.nivel)\n",
    "    print(\"Fortalezas:\", estado.fortalezas)\n",
    "    print(\"Debilidades:\", estado.debilidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa34f49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vamos a hacer un quiz para conocer tu nivel.\n",
      "\n",
      "--- Resultado crudo del LLM ---\n",
      "{'respuestas_usuario': \"[{'nivel': 'basico', 'tema': 'Distribuciones simples', 'pregunta': '¿Qué es una distribución uniforme?', 'respuesta': 'una distribucion donde la probabilidad es igual'}, {'nivel': 'intermedio', 'tema': 'Medidas de dispersión', 'pregunta': '¿Qué es la varianza?', 'respuesta': 'que tan alejados estan los datos del promedio'}, {'nivel': 'avanzado', 'tema': 'Distribuciones avanzadas', 'pregunta': '¿Qué es una distribución binomial?', 'respuesta': 'una distribucion donde los suceson solo pueden ser exito o fracaso'}]\", 'text': {'nivel': 'intermedio', 'fortalezas': ['Distribuciones simples', 'Medidas de dispersión'], 'debilidades': ['Distribuciones avanzadas'], 'detalle': [{'pregunta': '¿Qué es una distribución uniforme?', 'respuesta': 'una distribucion donde la probabilidad es igual', 'tema': 'Distribuciones simples', 'puntaje': 4, 'feedback': 'Correcto concepto básico, pero faltó mencionar que la probabilidad es constante sobre todo el dominio (discreto o continuo).'}, {'pregunta': '¿Qué es la varianza?', 'respuesta': 'que tan alejados estan los datos del promedio', 'tema': 'Medidas de dispersión', 'puntaje': 3, 'feedback': 'Describe la idea general de dispersión respecto al promedio, pero no menciona que se basa en la media de los cuadrados de las desviaciones.'}, {'pregunta': '¿Qué es una distribución binomial?', 'respuesta': 'una distribucion donde los suceson solo pueden ser exito o fracaso', 'tema': 'Distribuciones avanzadas', 'puntaje': 1, 'feedback': 'Define una distribución de Bernoulli individual. La binomial es la distribución de la suma de éxitos en n ensayos independientes.'}]}}\n",
      "\n",
      "Nivel detectado: intermedio\n",
      "Fortalezas: ['Distribuciones simples', 'Medidas de dispersión']\n",
      "Debilidades: ['Distribuciones avanzadas']\n",
      "\n",
      "Resumen final:\n",
      "Nivel: intermedio\n",
      "Fortalezas: ['Distribuciones simples', 'Medidas de dispersión']\n",
      "Debilidades: ['Distribuciones avanzadas']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "\n",
    "# 1. Clase Estado simple\n",
    "class Estado:\n",
    "    def __init__(self):\n",
    "        self.nivel = None\n",
    "        self.fortalezas = []\n",
    "        self.debilidades = []\n",
    "        self.detalle = []\n",
    "\n",
    "# 2. Preguntas por nivel y tema\n",
    "quiz_preguntas = {\n",
    "    \"basico\": [\n",
    "        {\"tema\": \"Variables aleatorias\", \"pregunta\": \"¿Qué es una variable aleatoria?\"},\n",
    "        {\"tema\": \"Probabilidad\", \"pregunta\": \"¿Qué es la probabilidad clásica?\"},\n",
    "        {\"tema\": \"Distribuciones simples\", \"pregunta\": \"¿Qué es una distribución uniforme?\"},\n",
    "        {\"tema\": \"Eventos\", \"pregunta\": \"¿Qué es un evento en probabilidad?\"}\n",
    "    ],\n",
    "    \"intermedio\": [\n",
    "        {\"tema\": \"Desviación estándar\", \"pregunta\": \"¿Qué es la desviación estándar?\"},\n",
    "        {\"tema\": \"Medidas de dispersión\", \"pregunta\": \"¿Qué es la varianza?\"},\n",
    "        {\"tema\": \"Estadística descriptiva\", \"pregunta\": \"¿Qué es la media aritmética?\"},\n",
    "        {\"tema\": \"Distribuciones\", \"pregunta\": \"¿Qué es una distribución normal?\"}\n",
    "    ],\n",
    "    \"avanzado\": [\n",
    "        {\"tema\": \"Probabilidad conjunta\", \"pregunta\": \"¿Cómo se calcula la probabilidad conjunta de dos eventos independientes?\"},\n",
    "        {\"tema\": \"Teorema de Bayes\", \"pregunta\": \"Explica el teorema de Bayes con un ejemplo.\"},\n",
    "        {\"tema\": \"Distribuciones avanzadas\", \"pregunta\": \"¿Qué es una distribución binomial?\"},\n",
    "        {\"tema\": \"Inferencia\", \"pregunta\": \"¿Qué es una estimación puntual en inferencia estadística?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 3. Template del prompt\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"Eres un experto en educación. Evalúa las siguientes respuestas del usuario a preguntas de probabilidad y estadística.\n",
    "Para cada respuesta, califica de 0 a 5 (donde 0 es incorrecta y 5 es perfecta), explica brevemente la calificación.\n",
    "Al final, resume las fortalezas y debilidades del usuario por tema y sugiere el nivel adecuado (básico, intermedio, avanzado) según el promedio de los puntajes:\n",
    "- Básico: promedio < 2.5\n",
    "- Intermedio: 2.5 <= promedio < 4\n",
    "- Avanzado: promedio >= 4\n",
    "\n",
    "Devuelve la respuesta SOLO en formato JSON con la siguiente estructura:\n",
    "{{\n",
    "  \"nivel\": \"basico/intermedio/avanzado\",\n",
    "  \"fortalezas\": [\"tema1\", \"tema2\", ...],\n",
    "  \"debilidades\": [\"tema1\", \"tema2\", ...],\n",
    "  \"detalle\": [\n",
    "    {{\n",
    "      \"pregunta\": \"...\",\n",
    "      \"respuesta\": \"...\",\n",
    "      \"tema\": \"...\",\n",
    "      \"puntaje\": 0-5,\n",
    "      \"feedback\": \"...\"\n",
    "    }},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Respuestas del usuario:\n",
    "{respuestas_usuario}\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# 4. Nodo de quiz de nivel usando LLMChain y JsonOutputParser\n",
    "def nodo_quiz_nivel(state):\n",
    "    print(\"\\nVamos a hacer un quiz para conocer tu nivel.\")\n",
    "    respuestas_usuario = []\n",
    "    for nivel, preguntas in quiz_preguntas.items():\n",
    "        seleccionadas = random.sample(preguntas, 1)  # 1 aleatoria por nivel\n",
    "        for q in seleccionadas:\n",
    "            resp = input(q[\"pregunta\"] + \" (responde brevemente): \")\n",
    "            respuestas_usuario.append({\n",
    "                \"nivel\": nivel,\n",
    "                \"tema\": q[\"tema\"],\n",
    "                \"pregunta\": q[\"pregunta\"],\n",
    "                \"respuesta\": resp\n",
    "            })\n",
    "\n",
    "    # 5. Prepara el LLM y la chain\n",
    "    llm = ChatOpenAI(model=\"o4-mini-2025-04-16\", temperature=1)\n",
    "    parser = JsonOutputParser()\n",
    "    chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt_template,\n",
    "        output_parser=parser\n",
    "    )\n",
    "\n",
    "    # 6. Ejecuta la chain\n",
    "    result = chain.invoke({\"respuestas_usuario\": str(respuestas_usuario)})\n",
    "\n",
    "    # 7. Imprime el resultado crudo para depuración\n",
    "    print(\"\\n--- Resultado crudo del LLM ---\")\n",
    "    print(result)\n",
    "\n",
    "    # 8. Si el resultado es un dict y tiene 'text', extrae los campos desde ahí\n",
    "    if isinstance(result, dict) and \"text\" in result:\n",
    "        data = result[\"text\"]\n",
    "        state.nivel = data[\"nivel\"]\n",
    "        state.fortalezas = data[\"fortalezas\"]\n",
    "        state.debilidades = data[\"debilidades\"]\n",
    "        state.detalle = data[\"detalle\"]\n",
    "        print(f\"\\nNivel detectado: {state.nivel}\")\n",
    "        print(f\"Fortalezas: {state.fortalezas}\")\n",
    "        print(f\"Debilidades: {state.debilidades}\")\n",
    "    else:\n",
    "        print(\"\\nNo se pudo extraer el JSON esperado. Revisa el resultado arriba.\")\n",
    "\n",
    "    return state\n",
    "\n",
    "# 5. Prueba el nodo\n",
    "if __name__ == \"__main__\":\n",
    "    estado = Estado()\n",
    "    nodo_quiz_nivel(estado)\n",
    "    print(\"\\nResumen final:\")\n",
    "    print(\"Nivel:\", estado.nivel)\n",
    "    print(\"Fortalezas:\", estado.fortalezas)\n",
    "    print(\"Debilidades:\", estado.debilidades)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd19b17",
   "metadata": {},
   "source": [
    "# Nodo plan de estudio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d5dc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plan de estudio personalizado:\n",
      "1. Probabilidad condicional  \n",
      "   • Repaso de definiciones y propiedades  \n",
      "   • Ley de probabilidades totales y ejercicios prácticos  \n",
      "\n",
      "2. Distribuciones de probabilidad  \n",
      "   • Discretas (Binomial, Poisson) y continuas (Normal, Exponencial)  \n",
      "   • Funciones de masa/densidad, esperanza y varianza  \n",
      "\n",
      "3. Teorema de Bayes  \n",
      "   • Formulación y vínculo con la ley de probabilidades totales  \n",
      "   • Aplicaciones reales: diagnósticos médicos, clasificación de eventos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Estado at 0x144b9c8f200>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI  # O el import de tu LLM preferido\n",
    "\n",
    "# Simulación de estado con debilidades\n",
    "class Estado:\n",
    "    def __init__(self, nivel, debilidades):\n",
    "        self.nivel = nivel\n",
    "        self.debilidades = debilidades\n",
    "        self.temas = []\n",
    "        self.tema_actual = 0\n",
    "\n",
    "# Prompt template para el plan de estudio\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Eres un tutor experto en estadística y probabilidad.\n",
    "El estudiante tiene el nivel: {nivel}.\n",
    "Sus debilidades principales son: {debilidades}.\n",
    "Crea un plan de estudio personalizado de exactamente 3 temas o pasos, enfocados en esas debilidades.\n",
    "Enumera los temas de forma clara y breve.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Instancia del LLM (ajusta el modelo según tu entorno)\n",
    "llm = ChatOpenAI(model=\"o4-mini-2025-04-16\", temperature=1)\n",
    "\n",
    "# Chain para generar el plan\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def nodo_plan_estudio(state: Estado) -> Estado:\n",
    "    # Llama al LLM para obtener el plan personalizado\n",
    "    debilidades_str = \", \".join(state.debilidades)\n",
    "    respuesta = chain.run(nivel=state.nivel, debilidades=debilidades_str)\n",
    "    print(\"\\nPlan de estudio personalizado:\")\n",
    "    print(respuesta)\n",
    "    # Extrae los temas del texto generado (puedes mejorar esto según el formato de respuesta)\n",
    "    temas = [line.strip(\"- \").strip() for line in respuesta.split(\"\\n\") if line.strip()]\n",
    "    state.temas = temas[:3]  # Solo los 3 primeros temas\n",
    "    state.tema_actual = 0\n",
    "    return state\n",
    "\n",
    "# Prueba aislada\n",
    "estado_prueba = Estado(nivel=\"intermedio\", debilidades=[\"probabilidad condicional\", \"distribuciones\", \"teorema de Bayes\"])\n",
    "nodo_plan_estudio(estado_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7df9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
